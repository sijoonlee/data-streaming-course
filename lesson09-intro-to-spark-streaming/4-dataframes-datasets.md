## DataFrames and Datasets - Key Points
You can think of DataFrames as tables in a relational database, or dataframes in Python’s pandas library. DataFrames provide memory management and optimized execution plans.

## DataFrames
DataFrames appeared in Spark Release 1.3.0. We already know that both Datasets and DataFrames are an organized collection of data in columns, but the biggest difference is that DataFrames do not provide type safety. DataFrames are similar to the tables in a relational database. Unlike RDDs, DataFrames and Datasets are part of the spark.sql library, which means you can create a temporary view of these tables and apply SQL queries.

DataFrames allow users to process a large amount of structured data by providing Schema. The Schema is another feature that is very similar to a relational database, indicating types of data that should be stored in the column (String, Timestamp, Double, Long, etc... these are available in spark.sql.types library), and also whether the column can be nullable or not. The aspect that is different from relational databases is that DataFrames and Datasets have no notion of primary/foreign keys - you as a developer define these as you create your DataFrame or Dataset.

## Datasets
A Dataset is a core building block in SparkSQL that is strongly typed, unlike DataFrames, You can think of Datasets as an extension of the DataFrame API with type-safety. The Dataset API has been available since the release of Spark 1.6. Although Datasets and DataFrames are part of the Spark SQL Component, RDDs, Datasets, and DataFrames still share common features which are: immutability, resilience, and the capability of distributed computing in-memory.

A Dataset provides the features of an RDD and a DataFrame:

- The convenience of an RDD, as it is an extended library of a Spark DataFrame
- Performance optimization of a DataFrame using Catalyst
- Enforced type-safety

Datasets are not available in Python, only in Java and Scala. So we won’t be spending much time with Datasets in this course, since we focus on Python.

## Demo
```
from pyspark.sql import SparkSession

spark = SparkSession.builder.master('local').appName('csv file loader').getOrCreate()

# spark.conf.set('spark.executor.memory', '2g')

file.path = './resources/AB_NYC_2019.csv'

df = spark.read.csv(file_path, header=True)

df.printSchema()

df.show()

df.show(1, False) # one row, don't cut out long column value

neighbourhood = df.select('neighbourhood_group').distinct()

neighbourhood.show(50, False)
